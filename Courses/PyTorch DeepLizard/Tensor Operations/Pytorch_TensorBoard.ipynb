{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Suggestion: Fix conflicting installations\n",
    "\n",
    "Conflicting package installations found. Depending on the order of\n",
    "installations and uninstallations, behavior may be undefined. Please\n",
    "uninstall ALL versions of TensorFlow and TensorBoard, then reinstall\n",
    "ONLY the desired version of TensorFlow, which will transitively pull\n",
    "in the proper version of TensorBoard. (If you use TensorBoard without\n",
    "TensorFlow, just reinstall the appropriate version of TensorBoard\n",
    "directly.)\n",
    "\n",
    "Namely:\n",
    "\n",
    "\tpip uninstall tb-nightly tensorboard tensorflow-estimator tensorflow-gpu tf-estimator-nightly\n",
    "\tpip install tensorflow  # or `tensorflow-gpu`, or `tf-nightly`, ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets, I have already downloaded\n",
    "trainset = torchvision.datasets.FashionMNIST('~/.pytorch',\n",
    "    download=True, train=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('~/.pytorch',\n",
    "    download=True, train=False, transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 1000\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(torch.permute(npimg, (1, 2, 0)).numpy())"
   ],
   "outputs": [],
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "tb = SummaryWriter('runs/fashion_mnist_experiment_1')"
   ],
   "outputs": [],
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# runnig on http://localhost:6006/\n",
    "# tensorboard --logdir=runs # copy to commandline"
   ],
   "outputs": [],
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        \n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds),\n",
    "            dim=0\n",
    "        )\n",
    "    return all_preds"
   ],
   "outputs": [],
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "outputHidden": false,
    "inputHidden": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for name, weight in net.named_parameters():\n",
    "    print(name, weight.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.weight torch.Size([6, 1, 5, 5])\n",
      "conv1.bias torch.Size([6])\n",
      "conv2.weight torch.Size([16, 6, 5, 5])\n",
      "conv2.bias torch.Size([16])\n",
      "fc1.weight torch.Size([120, 256])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([84, 120])\n",
      "fc2.bias torch.Size([84])\n",
      "fc3.weight torch.Size([10, 84])\n",
      "fc3.bias torch.Size([10])\n"
     ]
    }
   ],
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "outputHidden": false,
    "inputHidden": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from itertools import product"
   ],
   "outputs": [],
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "outputHidden": false,
    "inputHidden": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 2 * 3 * 2 = 12 different training sessions\n",
    "parameters = dict(\n",
    "    lr = [.01, .001],\n",
    "    batch_size = [10, 100, 1000],\n",
    "    shuffle = [True, False]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "outputHidden": false,
    "inputHidden": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "param_values = [v for v in parameters.values()]\n",
    "param_values"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 9,
     "data": {
      "text/plain": [
       "[[0.01, 0.001], [10, 100, 1000], [True, False]]"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "outputHidden": false,
    "inputHidden": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# let's see what is goning on.\n",
    "for lr, batch_size, shuffle in product(*param_values):\n",
    "    print(lr, batch_size, shuffle)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.01 10 True\n",
      "0.01 10 False\n",
      "0.01 100 True\n",
      "0.01 100 False\n",
      "0.01 1000 True\n",
      "0.01 1000 False\n",
      "0.001 10 True\n",
      "0.001 10 False\n",
      "0.001 100 True\n",
      "0.001 100 False\n",
      "0.001 1000 True\n",
      "0.001 1000 False\n"
     ]
    }
   ],
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "outputHidden": false,
    "inputHidden": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# let's see what is goning on.\n",
    "for lr, batch_size, shuffle in product(*param_values): \n",
    "    comment = f' batch_size={batch_size} lr={lr} shuffle={shuffle}'\n",
    "\n",
    "    # Training process given the set of parameters"
   ],
   "outputs": [],
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "outputHidden": false,
    "inputHidden": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "batch_size_list=[100, 1000, 10000]\n",
    "lr_list=[.01, .001, .0001, .00001]\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "outputHidden": false,
    "inputHidden": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# loop over for batch_size and lr_list hyper parameter\n",
    "for lr, batch_size, shuffle in tqdm(product(*param_values), 'hyper parameter testing'):\n",
    "    net = Net()\n",
    "    # lr = 0.001\n",
    "    trainset = torchvision.datasets.FashionMNIST('~/.pytorch',\n",
    "    download=True, train=True, transform=transform)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    #Not necessary. Test it.\n",
    "    images, labels = next(iter(trainloader))\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "    comment = f' batch_size={batch_size} lr={lr} shuffle={shuffle}'\n",
    "    tb=SummaryWriter(comment=comment)\n",
    "    #Not necessary. Test it.\n",
    "    tb.add_image('images', grid)\n",
    "    tb.add_graph(net, images)\n",
    "\n",
    "    # main training loop\n",
    "    for epoch in tqdm(range(epochs), 'Training'):\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch in trainloader: # Get Batch\n",
    "            # Pass Batch\n",
    "            images, labels = batch\n",
    "            # forward pass\n",
    "            outputs = net(images)\n",
    "            # Calculate Loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Calculate Gradient\n",
    "            loss.backward()\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # or total_loss+=loss.item()*images.shape[0]\n",
    "            total_loss += loss.item() * batch_size # for comporable batch sizes\n",
    "            total_correct += get_num_correct(outputs, labels)\n",
    "\n",
    "        # epoch <-> epoch * len(trainloader) + i\n",
    "        tb.add_scalar('Loss', total_loss, epoch)\n",
    "        tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "        tb.add_scalar('Accuracy', total_correct / len(trainset), epoch)\n",
    "\n",
    "        #tb.add_histogram('conv1.bias', net.conv1.bias, epoch)\n",
    "        #tb.add_histogram('conv1.weight', net.conv1.weight, epoch)\n",
    "        #tb.add_histogram('conv1.weight.grad', net.conv1.weight.grad, epoch)\n",
    "        for name, weight in net.named_parameters():\n",
    "            tb.add_histogram(name, weight, epoch)\n",
    "            tb.add_histogram(f'{name}.grad', weight.grad, epoch) # grads also has same shape\n",
    "\n",
    "        print(\"epoch\", epoch, \n",
    "              \"total_correct:\", total_correct, \n",
    "              \"loss:\", total_loss\n",
    "              )\n",
    "\n",
    "    tb.close()\n",
    "print(\"training finished\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "hyper parameter testing: 0it [00:00, ?it/s]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:11<01:43, 11.46s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:22<01:30, 11.34s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:33<01:18, 11.28s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:45<01:07, 11.33s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:56<00:56, 11.26s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:07<00:45, 11.28s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:18<00:33, 11.18s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:29<00:22, 11.17s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:40<00:11, 11.15s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:51<00:00, 11.18s/it]\u001b[A\n",
      "hyper parameter testing: 1it [01:54, 114.58s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:11<01:40, 11.21s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:22<01:29, 11.24s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:33<01:18, 11.28s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:45<01:07, 11.23s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:56<00:55, 11.17s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:07<00:44, 11.16s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:18<00:33, 11.09s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:29<00:22, 11.08s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:40<00:11, 11.14s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:51<00:00, 11.14s/it]\u001b[A\n",
      "hyper parameter testing: 2it [03:48, 114.42s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:10<01:38, 10.95s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:22<01:29, 11.24s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:33<01:18, 11.17s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:44<01:06, 11.06s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:55<00:54, 10.98s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:07<00:44, 11.18s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:18<00:33, 11.27s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:29<00:22, 11.20s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:40<00:11, 11.16s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:51<00:00, 11.17s/it]\u001b[A\n",
      "hyper parameter testing: 3it [05:43, 114.41s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:11<01:40, 11.14s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:22<01:28, 11.11s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:33<01:17, 11.08s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:44<01:06, 11.08s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:55<00:55, 11.06s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:06<00:44, 11.03s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:17<00:33, 11.04s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:28<00:21, 10.98s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:39<00:11, 11.02s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:50<00:00, 11.03s/it]\u001b[A\n",
      "hyper parameter testing: 4it [07:35, 113.94s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:11<01:41, 11.25s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:22<01:29, 11.19s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:33<01:18, 11.18s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:44<01:07, 11.17s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:55<00:56, 11.24s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:06<00:44, 11.16s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:18<00:33, 11.13s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:28<00:22, 11.05s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:39<00:10, 10.97s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:50<00:00, 11.06s/it]\u001b[A\n",
      "hyper parameter testing: 5it [09:29, 113.71s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:11<01:39, 11.07s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:22<01:29, 11.18s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:33<01:17, 11.13s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:44<01:06, 11.04s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:55<00:54, 10.96s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:06<00:43, 10.96s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:16<00:32, 10.90s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:27<00:21, 10.94s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:38<00:10, 10.92s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:49<00:00, 10.96s/it]\u001b[A\n",
      "hyper parameter testing: 6it [11:21, 113.20s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:10<01:37, 10.80s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:21<01:25, 10.75s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:32<01:15, 10.72s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:42<01:04, 10.76s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:53<00:53, 10.73s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:04<00:42, 10.73s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:15<00:32, 10.76s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:25<00:21, 10.76s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:36<00:10, 10.75s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:47<00:00, 10.74s/it]\u001b[A\n",
      "hyper parameter testing: 7it [13:10, 112.21s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:10<01:35, 10.60s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:21<01:25, 10.69s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:32<01:14, 10.68s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:42<01:03, 10.67s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:53<00:53, 10.72s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:04<00:43, 10.76s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:15<00:32, 10.74s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:25<00:21, 10.74s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:36<00:10, 10.74s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:47<00:00, 10.74s/it]\u001b[A\n",
      "hyper parameter testing: 8it [15:00, 111.53s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:10<01:35, 10.58s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:21<01:24, 10.59s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:32<01:14, 10.68s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:42<01:04, 10.67s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:53<00:53, 10.66s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:04<00:43, 10.79s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:15<00:32, 10.75s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:26<00:21, 10.84s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:37<00:10, 10.87s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:47<00:00, 10.79s/it]\u001b[A\n",
      "hyper parameter testing: 9it [16:51, 111.16s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:10<01:38, 10.94s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:22<01:28, 11.05s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:33<01:17, 11.14s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:44<01:07, 11.20s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:55<00:55, 11.10s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:07<00:44, 11.24s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:19<00:34, 11.42s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:30<00:22, 11.39s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:41<00:11, 11.21s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:52<00:00, 11.21s/it]\u001b[A\n",
      "hyper parameter testing: 10it [18:45, 112.20s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:11<01:43, 11.50s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:22<01:31, 11.45s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:34<01:19, 11.38s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:45<01:08, 11.35s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:57<00:57, 11.50s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:08<00:45, 11.40s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:19<00:34, 11.41s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:31<00:22, 11.36s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:42<00:11, 11.25s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [01:53<00:00, 11.37s/it]\u001b[A\n",
      "hyper parameter testing: 11it [20:42, 113.45s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:11<01:41, 11.31s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:22<01:30, 11.35s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:35<01:22, 11.83s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:47<01:11, 11.85s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:59<00:59, 11.91s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [01:11<00:48, 12.02s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [01:23<00:36, 12.00s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [01:35<00:23, 11.98s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [01:49<00:12, 12.51s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [02:02<00:00, 12.24s/it]\u001b[A\n",
      "hyper parameter testing: 12it [22:47, 113.93s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 total_correct: 39257 loss: 541.1765885353088\n",
      "epoch 1 total_correct: 49602 loss: 277.9078522324562\n",
      "epoch 2 total_correct: 51468 loss: 229.14370715618134\n",
      "epoch 3 total_correct: 52532 loss: 201.99220895767212\n",
      "epoch 4 total_correct: 53102 loss: 185.3171768784523\n",
      "epoch 5 total_correct: 53491 loss: 176.01554304361343\n",
      "epoch 6 total_correct: 53928 loss: 162.99029365181923\n",
      "epoch 7 total_correct: 53885 loss: 163.12776997685432\n",
      "epoch 8 total_correct: 54306 loss: 152.14353054761887\n",
      "epoch 9 total_correct: 54592 loss: 145.2783463895321\n",
      "epoch 0 total_correct: 40097 loss: 516.8755614757538\n",
      "epoch 1 total_correct: 49363 loss: 277.67229348421097\n",
      "epoch 2 total_correct: 51513 loss: 226.73743546009064\n",
      "epoch 3 total_correct: 52419 loss: 205.15164077281952\n",
      "epoch 4 total_correct: 52953 loss: 189.8842516541481\n",
      "epoch 5 total_correct: 53443 loss: 177.29526296257973\n",
      "epoch 6 total_correct: 53815 loss: 168.05337235331535\n",
      "epoch 7 total_correct: 54125 loss: 157.8394030034542\n",
      "epoch 8 total_correct: 54305 loss: 155.16435906291008\n",
      "epoch 9 total_correct: 54406 loss: 150.09694516658783\n",
      "epoch 0 total_correct: 39947 loss: 5213.503819704056\n",
      "epoch 1 total_correct: 49544 loss: 2756.8893432617188\n",
      "epoch 2 total_correct: 51422 loss: 2310.4694694280624\n",
      "epoch 3 total_correct: 52394 loss: 2044.2296773195267\n",
      "epoch 4 total_correct: 52848 loss: 1908.6260110139847\n",
      "epoch 5 total_correct: 53323 loss: 1798.0532929301262\n",
      "epoch 6 total_correct: 53491 loss: 1720.3694701194763\n",
      "epoch 7 total_correct: 53780 loss: 1671.9282910227776\n",
      "epoch 8 total_correct: 53805 loss: 1647.7254509925842\n",
      "epoch 9 total_correct: 54270 loss: 1524.61688965559\n",
      "epoch 0 total_correct: 41008 loss: 5035.726109147072\n",
      "epoch 1 total_correct: 50065 loss: 2666.469618678093\n",
      "epoch 2 total_correct: 51991 loss: 2185.0332349538803\n",
      "epoch 3 total_correct: 52689 loss: 1971.2925493717194\n",
      "epoch 4 total_correct: 53428 loss: 1789.1608774662018\n",
      "epoch 5 total_correct: 53641 loss: 1708.9328229427338\n",
      "epoch 6 total_correct: 53971 loss: 1628.128582239151\n",
      "epoch 7 total_correct: 54290 loss: 1535.9720423817635\n",
      "epoch 8 total_correct: 54440 loss: 1484.2794224619865\n",
      "epoch 9 total_correct: 54610 loss: 1419.2017033696175\n",
      "epoch 0 total_correct: 40223 loss: 52200.503796339035\n",
      "epoch 1 total_correct: 49849 loss: 27280.95105290413\n",
      "epoch 2 total_correct: 51817 loss: 22299.948900938034\n",
      "epoch 3 total_correct: 52541 loss: 20169.784277677536\n",
      "epoch 4 total_correct: 53066 loss: 18700.98379254341\n",
      "epoch 5 total_correct: 53547 loss: 17481.527268886566\n",
      "epoch 6 total_correct: 53853 loss: 16510.456264019012\n",
      "epoch 7 total_correct: 54099 loss: 15942.994311451912\n",
      "epoch 8 total_correct: 54333 loss: 15271.83224260807\n",
      "epoch 9 total_correct: 54576 loss: 14414.336189627647\n",
      "epoch 0 total_correct: 40581 loss: 49810.998022556305\n",
      "epoch 1 total_correct: 50193 loss: 26299.08326268196\n",
      "epoch 2 total_correct: 52002 loss: 21681.632459163666\n",
      "epoch 3 total_correct: 52798 loss: 19502.110719680786\n",
      "epoch 4 total_correct: 53275 loss: 18112.735271453857\n",
      "epoch 5 total_correct: 53805 loss: 16740.10221660137\n",
      "epoch 6 total_correct: 53919 loss: 16431.97950720787\n",
      "epoch 7 total_correct: 54260 loss: 15350.150808691978\n",
      "epoch 8 total_correct: 54577 loss: 14504.211232066154\n",
      "epoch 9 total_correct: 54670 loss: 14245.173022150993\n",
      "epoch 0 total_correct: 31200 loss: 835.1027452945709\n",
      "epoch 1 total_correct: 45057 loss: 393.9842349290848\n",
      "epoch 2 total_correct: 47014 loss: 338.8849139213562\n",
      "epoch 3 total_correct: 48246 loss: 311.449753344059\n",
      "epoch 4 total_correct: 49456 loss: 286.08761101961136\n",
      "epoch 5 total_correct: 50144 loss: 270.4256671667099\n",
      "epoch 6 total_correct: 50905 loss: 252.97940850257874\n",
      "epoch 7 total_correct: 51176 loss: 244.06404852867126\n",
      "epoch 8 total_correct: 51528 loss: 234.3623462319374\n",
      "epoch 9 total_correct: 51811 loss: 226.99802309274673\n",
      "epoch 0 total_correct: 34153 loss: 792.7210640907288\n",
      "epoch 1 total_correct: 44988 loss: 400.4547595977783\n",
      "epoch 2 total_correct: 46785 loss: 345.2343487739563\n",
      "epoch 3 total_correct: 48018 loss: 312.9770255088806\n",
      "epoch 4 total_correct: 49081 loss: 292.96543926000595\n",
      "epoch 5 total_correct: 50070 loss: 270.1954448223114\n",
      "epoch 6 total_correct: 50766 loss: 254.89858865737915\n",
      "epoch 7 total_correct: 51186 loss: 244.0894564986229\n",
      "epoch 8 total_correct: 51521 loss: 233.80095839500427\n",
      "epoch 9 total_correct: 51787 loss: 227.3188692331314\n",
      "epoch 0 total_correct: 31224 loss: 8664.122462272644\n",
      "epoch 1 total_correct: 43980 loss: 4235.4218780994415\n",
      "epoch 2 total_correct: 46644 loss: 3490.2314364910126\n",
      "epoch 3 total_correct: 47937 loss: 3147.631323337555\n",
      "epoch 4 total_correct: 48791 loss: 2976.695218682289\n",
      "epoch 5 total_correct: 49630 loss: 2798.649325966835\n",
      "epoch 6 total_correct: 50369 loss: 2655.2370190620422\n",
      "epoch 7 total_correct: 50973 loss: 2519.308477640152\n",
      "epoch 8 total_correct: 51375 loss: 2410.7202500104904\n",
      "epoch 9 total_correct: 51665 loss: 2330.8774322271347\n",
      "epoch 0 total_correct: 32494 loss: 8260.446310043335\n",
      "epoch 1 total_correct: 44008 loss: 4091.265708208084\n",
      "epoch 2 total_correct: 46448 loss: 3511.6869807243347\n",
      "epoch 3 total_correct: 47890 loss: 3223.031085729599\n",
      "epoch 4 total_correct: 48833 loss: 2990.5300617218018\n",
      "epoch 5 total_correct: 49562 loss: 2841.9478833675385\n",
      "epoch 6 total_correct: 50214 loss: 2676.3933777809143\n",
      "epoch 7 total_correct: 50614 loss: 2576.998648047447\n",
      "epoch 8 total_correct: 50819 loss: 2508.4744930267334\n",
      "epoch 9 total_correct: 51334 loss: 2388.1713658571243\n",
      "epoch 0 total_correct: 34705 loss: 75818.96531581879\n",
      "epoch 1 total_correct: 45050 loss: 39075.39039850235\n",
      "epoch 2 total_correct: 47114 loss: 33606.9572865963\n",
      "epoch 3 total_correct: 48478 loss: 30640.72135090828\n",
      "epoch 4 total_correct: 49585 loss: 28525.294095277786\n",
      "epoch 5 total_correct: 50444 loss: 26673.381716012955\n",
      "epoch 6 total_correct: 50823 loss: 25471.231520175934\n",
      "epoch 7 total_correct: 51220 loss: 24325.200736522675\n",
      "epoch 8 total_correct: 51550 loss: 23463.267385959625\n",
      "epoch 9 total_correct: 51855 loss: 22679.7032058239\n",
      "epoch 0 total_correct: 33686 loss: 77039.35521841049\n",
      "epoch 1 total_correct: 44570 loss: 40293.41417551041\n",
      "epoch 2 total_correct: 46445 loss: 35273.00250530243\n",
      "epoch 3 total_correct: 47839 loss: 31861.5220785141\n",
      "epoch 4 total_correct: 48970 loss: 29536.415815353394\n",
      "epoch 5 total_correct: 49776 loss: 27831.46780729294\n",
      "epoch 6 total_correct: 50479 loss: 26208.909302949905\n",
      "epoch 7 total_correct: 50925 loss: 24798.936784267426\n",
      "epoch 8 total_correct: 51345 loss: 23859.163284301758\n",
      "epoch 9 total_correct: 51673 loss: 23007.643938064575\n"
     ]
    }
   ],
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernel_info": {
   "name": "python3"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}