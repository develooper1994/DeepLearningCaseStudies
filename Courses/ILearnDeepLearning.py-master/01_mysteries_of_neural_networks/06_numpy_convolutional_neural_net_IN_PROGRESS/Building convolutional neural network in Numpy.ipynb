{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building convolutional neural network in Numpy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author: Piotr Skalski***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "# size of the photo\n",
    "PHOTO_SIZE = 28\n",
    "# number of pixels in the photo\n",
    "PIXEL_NUMBER = PHOTO_SIZE * PHOTO_SIZE\n",
    "# neural network architecture\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": PIXEL_NUMBER, \"output_dim\": 1000, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 1000, \"output_dim\": 1000, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 1000, \"output_dim\": 500, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 500, \"output_dim\": 500, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 500, \"output_dim\": 10, \"activation\": \"softmax\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist_dataset():\n",
    "    # The MNIST data set is available at http://yann.lecun.com, let's use curl to download it\n",
    "    if not os.path.exists(\"train-images-idx3-ubyte\"):\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "        !gunzip t*-ubyte.gz\n",
    "        \n",
    "    # Let's use loadlocal_mnist available in mlxtend.data to get data in numpy array form.\n",
    "    X1, y1 = loadlocal_mnist(\n",
    "        images_path=\"train-images-idx3-ubyte\", \n",
    "        labels_path=\"train-labels-idx1-ubyte\")\n",
    "\n",
    "    X2, y2 = loadlocal_mnist(\n",
    "        images_path=\"t10k-images-idx3-ubyte\", \n",
    "        labels_path=\"t10k-labels-idx1-ubyte\")\n",
    "    \n",
    "    # We normalize the brightness values for pixels\n",
    "    X1 = X1.reshape(X1.shape[0], -1) / 255\n",
    "    X2 = X2.reshape(X2.shape[0], -1) /255\n",
    "\n",
    "    # Combine downloaded data bundles\n",
    "    X = np.concatenate([X1, X2])\n",
    "    y = np.concatenate([y1, y2])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function for labels one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y):\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(split_percentage):\n",
    "    # Download data\n",
    "    X, y = download_mnist_dataset()\n",
    "    # One hot encode labels\n",
    "    y = one_hot_encoding(y)\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_percentage, random_state=42)\n",
    "    return np.transpose(X_train), np.transpose(X_test), np.transpose(y_train), np.transpose(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function to display the selected data set element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(X, y, idx):\n",
    "    img = X[:, idx].reshape(28,28)\n",
    "    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
    "    plt.title('true label: %d' % np.where(y[:, idx] != 0)[0][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5c5a65d0ddd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_digit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "plot_digit(X_train, y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation of the existing implementation to support multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    # Column wise softmax\n",
    "    e_Z = np.exp(Z - np.max(Z))\n",
    "    return e_Z / e_Z.sum(axis = 0)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2D = np.array([[1, 2, 3, 6],\n",
    "                     [2, 4, 5, 6],\n",
    "                     [3, 8, 7, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09003057, 0.00242826, 0.01587624, 0.33333333],\n",
       "       [0.24472847, 0.01794253, 0.11731043, 0.33333333],\n",
       "       [0.66524096, 0.97962921, 0.86681333, 0.33333333]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(scores2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    elif activation is \"softmax\":\n",
    "        activation_func = softmax\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cost_value(Y_hat, Y, eps = 0.001):\n",
    "#     m = Y_hat.shape[1]\n",
    "#     cost = -1 / m * (np.dot(Y, np.log(Y_hat + eps).T) + np.dot(1 - Y, np.log(1 - Y_hat  + eps).T))\n",
    "#     return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_cross_entropy_loss(Y_hat, Y):\n",
    "    m = Y_hat.shape[1]\n",
    "    loss = - np.sum(Y * np.log(Y_hat)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_prob_into_class(probs):\n",
    "#     probs_ = np.copy(probs)\n",
    "#     probs_[probs_ > 0.5] = 1\n",
    "#     probs_[probs_ <= 0.5] = 0\n",
    "#     return probs_\n",
    "\n",
    "\n",
    "# def get_accuracy_value(Y_hat, Y):\n",
    "#     Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "#     return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_accuracy(Y_hat, Y):\n",
    "    n_values = Y_hat.shape[0]\n",
    "    values = Y_hat.argmax(axis=0)\n",
    "    Y_hat_one_hot = np.eye(n_values)[values].T\n",
    "    return (Y_hat_one_hot == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    elif activation is \"softmax\":\n",
    "        # TEST OF COURSERA SOLUTION\n",
    "        backward_activation_func = lambda dA_curr, Z_curr: dA_curr\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture, eps = 0.000000000001):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "#     dA_prev = - (np.divide(Y, Y_hat + eps) - np.divide(1 - Y, 1 - Y_hat + eps))\n",
    "    # TEST OF COURSERA SOLUTION\n",
    "    dA_prev = Y_hat - Y\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        \n",
    "#         print(Y_hat.shape)\n",
    "#         print(Y.shape)\n",
    "        \n",
    "        cost = multi_class_cross_entropy_loss(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "#         print(cost)\n",
    "        \n",
    "        accuracy = multi_class_accuracy(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "#         print(accuracy)\n",
    "        \n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "        if(i % 5 == 0):\n",
    "            if(verbose):\n",
    "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            if(callback is not None):\n",
    "                callback(i, params_values)\n",
    "            \n",
    "    return params_values, cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SIZE = 10000\n",
    "X_train = X_train[:,:SUBSET_SIZE]\n",
    "y_train = y_train[:,:SUBSET_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 - cost: 18.05769 - accuracy: 0.09350\n",
      "Iteration: 00005 - cost: 6.55857 - accuracy: 0.34620\n",
      "Iteration: 00010 - cost: 1.29068 - accuracy: 0.67030\n",
      "Iteration: 00015 - cost: 1.01207 - accuracy: 0.73030\n",
      "Iteration: 00020 - cost: 0.85441 - accuracy: 0.76560\n",
      "Iteration: 00025 - cost: 0.75597 - accuracy: 0.78840\n",
      "Iteration: 00030 - cost: 0.68273 - accuracy: 0.80950\n",
      "Iteration: 00035 - cost: 0.62498 - accuracy: 0.82370\n",
      "Iteration: 00040 - cost: 0.57798 - accuracy: 0.83580\n",
      "Iteration: 00045 - cost: 0.53846 - accuracy: 0.84670\n",
      "Iteration: 00050 - cost: 0.50445 - accuracy: 0.85660\n",
      "Iteration: 00055 - cost: 0.47479 - accuracy: 0.86430\n",
      "Iteration: 00060 - cost: 0.44866 - accuracy: 0.87120\n",
      "Iteration: 00065 - cost: 0.42536 - accuracy: 0.87710\n",
      "Iteration: 00070 - cost: 0.40445 - accuracy: 0.88430\n",
      "Iteration: 00075 - cost: 0.38556 - accuracy: 0.88860\n",
      "Iteration: 00080 - cost: 0.36836 - accuracy: 0.89520\n",
      "Iteration: 00085 - cost: 0.35262 - accuracy: 0.89900\n",
      "Iteration: 00090 - cost: 0.33812 - accuracy: 0.90310\n",
      "Iteration: 00095 - cost: 0.32469 - accuracy: 0.90650\n"
     ]
    }
   ],
   "source": [
    "params_values, cost_history, accuracy_history = train(X_train, y_train, NN_ARCHITECTURE, 100, 0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8602857142857143\n"
     ]
    }
   ],
   "source": [
    "y_test_hat, _ = full_forward_propagation(X_test, params_values, NN_ARCHITECTURE)\n",
    "accuracy = multi_class_accuracy(y_test_hat, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "http://wiki.fast.ai/index.php/Log_Loss\n",
    "http://cs231n.github.io/neural-networks-case-study/\n",
    "http://saitcelebi.com/tut/output/part2.html\n",
    "https://deepnotes.io/softmax-crossentropy\n",
    "https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
