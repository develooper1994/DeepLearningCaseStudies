{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    ":label:`sec_pandas`\n",
    "\n",
    "\n",
    "So far we have introduced a variety of techniques for manipulating data that are already stored in `ndarray`s.\n",
    "To apply deep learning to solving real-world problems,\n",
    "we often begin with preprocessing raw data, rather than those nicely prepared data in the `ndarray` format.\n",
    "Among popular data analytic tools in Python, the `pandas` package is commonly used.\n",
    "Like many other extension packages in the vast ecosystem of Python,\n",
    "`pandas` can work together with `ndarray`.\n",
    "So, we will briefly walk through steps for preprocessing raw data with `pandas`\n",
    "and converting them into the `ndarray` format.\n",
    "We will cover more data preprocessing techniques in later chapters.\n",
    "\n",
    "## Reading the Dataset\n",
    "\n",
    "As an example, we begin by creating an artificial dataset that is stored in a csv (comma-separated values) file `../data/house_tiny.csv`. Data stored in other formats may be processed in similar ways. The following `mkdir_if_not_exist` function ensures that the directory `../data` exists. The comment `# Saved in the d2l package for later use` is a special mark where the following function, class, or import statements\n",
    "are also saved in the `d2l` package so that we can directly invoke `d2l.mkdir_if_not_exist()` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Saved in the d2l package for later use\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not isinstance(path, str):\n",
    "        path = os.path.join(*path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write the dataset row by row into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/house_tiny.csv'\n",
    "mkdir_if_not_exist('../data')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
    "    f.write('NA,Pave,127500\\n')  # Each row is a data point\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the raw dataset from the created csv file,\n",
    "we import the `pandas` package and invoke the `read_csv` function.\n",
    "This dataset has $4$ rows and $3$ columns, where each row describes the number of rooms (\"NumRooms\"), the alley type (\"Alley\"), and the price (\"Price\") of a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "# If pandas is not installed, just uncomment the following line:\n",
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Note that \"NaN\" entries are missing values.\n",
    "To handle missing data, typical methods include *imputation* and *deletion*,\n",
    "where imputation replaces missing values with substituted ones,\n",
    "while deletion ignores missing values. Here we will consider imputation.\n",
    "\n",
    "By integer-location based indexing (`iloc`), we split `data` into `inputs` and `outputs`,\n",
    "where the former takes the first 2 columns while the later only keeps the last column.\n",
    "For numerical values in `inputs` that are missing, we replace the \"NaN\" entries with the mean value of the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "1       2.0   NaN\n",
      "2       4.0   NaN\n",
      "3       3.0   NaN\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical or discrete values in `inputs`, we consider \"NaN\" as a category.\n",
    "Since the \"Alley\" column only takes 2 types of categorical values \"Pave\" and \"NaN\",\n",
    "`pandas` can automatically convert this column to 2 columns \"Alley_Pave\" and \"Alley_nan\".\n",
    "A row whose alley type is \"Pave\" will set values of \"Alley_Pave\" and \"Alley_nan\" to $1$ and $0$.\n",
    "A row with a missing alley type will set their values to $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0           1          0\n",
      "1       2.0           0          1\n",
      "2       4.0           0          1\n",
      "3       3.0           0          1\n"
     ]
    }
   ],
   "source": [
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to the  `ndarray` Format\n",
    "\n",
    "Now that all the entries in `inputs` and `outputs` are numerical, they can be converted to the `ndarray` format.\n",
    "Once data are in this format, they can be further manipulated with those `ndarray` functionalities that we have introduced in :numref:`sec_ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3., 1., 0.],\n",
       "        [2., 0., 1.],\n",
       "        [4., 0., 1.],\n",
       "        [3., 0., 1.]], dtype=float64),\n",
       " array([127500, 106000, 178100, 140000], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import np\n",
    "\n",
    "X, y = np.array(inputs.values), np.array(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Like many other extension packages in the vast ecosystem of Python, `pandas` can work together with `ndarray`.\n",
    "* Imputation and deletion can be used to handle missing data.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Create a raw dataset with more rows and columns.\n",
    "\n",
    "1. Delete the column with the most missing values.\n",
    "2. Convert the preprocessed dataset to the `ndarray` format.\n",
    "\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/4973)\n",
    "\n",
    "![](../img/qr_pandas.svg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}