{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Capsule Network\n",
        "\n",
        "In this notebook, I'll be building a simple Capsule Network that aims to classify MNIST images. This is an implementation in PyTorch and this notebook assumes that you are already familiar with [convolutional and fully-connected layers](https://cezannec.github.io/Convolutional_Neural_Networks/). \n",
        "\n",
        "### What are Capsules?\n",
        "\n",
        "Capsules are a small group of neurons that have a few key traits:\n",
        ">* Each neuron in a capsule represents various properties of a particular image part; properties like a parts color, width, etc. \n",
        "* Every capsule **outputs a vector**, which has some magnitude (that represents the probability of a part's **existence**) and orientation (that represents a part's generalized pose).\n",
        "* A capsule network is made of multiple layers of capsules; during training, this network aims to learn the spatial relationships between the parts and whole of an object (ex. how the position of eyes and a nose relate to the position of a whole face in an image).\n",
        "* Capsules represent relationships between parts of a whole object by using **dynamic routing** to weight the connections between one layer of capsules and the next and creating strong connections between spatially-related object parts. \n",
        "\n",
        "<img src='assets/cat_face_2.png' width=50% />\n",
        "\n",
        "You can read more about all of these traits in [my blog post about capsules and dynamic routing](https://cezannec.github.io/Capsule_Networks/).\n",
        "\n",
        "### Representing Relationships Between Parts\n",
        "\n",
        "All of these traits allow capsules to communicate with each other and determine how data moves through them. Using dynamic communication, during the training process, a capsule network learns the **spatial relationships** between visual parts and their wholes (ex. between eyes, a nose, and a mouth on a face). When compared to a vanilla CNN, this knowledge about spatial relationships makes it easier for a capsule network to identify an object no matter what orientation it is in. These networks are also better able to identify multiple, overlapping objects, and learn from smaller sets of training data! \n",
        "\n",
        "---\n",
        "## Model Architecture\n",
        "\n",
        "The Capsule Network that I'll define is made of two main parts:\n",
        "1. A convolutional encoder\n",
        "2. A fully-connected, linear decoder\n",
        "\n",
        "I'll be following the architecture described [in the original Capsule Network paper](https://arxiv.org/pdf/1710.09829.pdf). I'll describe each layer in more detail as I define it in this notebook, but first let's load in our resources and MNIST data. I'm also setting a random seed, for reproducibility."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# import resources\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# random seed (for reproducibility)\n",
        "seed = 1\n",
        "# set random seed for numpy\n",
        "np.random.seed(seed)\n",
        "# set random seed for pytorch\n",
        "torch.manual_seed(seed)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x23b700d9c70>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtain the [Data](http://pytorch.org/docs/stable/torchvision/datasets.html)\n",
        "\n",
        "Downloading may take a few moments, and you should see your progress as the data is loading. You may also choose to change the `batch_size` parameter, if you want to load more data at a time, but the included parameters are fine to leave as is.\n",
        "\n",
        "This cell will create `DataLoaders` for the training and test sets."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# convert data to Tensors\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                            download=True, transform=transform)\n",
        "\n",
        "test_data = datasets.MNIST(root='data', train=False, \n",
        "                           download=True, transform=transform)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, \n",
        "                                           batch_size=batch_size, \n",
        "                                           num_workers=num_workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, \n",
        "                                          batch_size=batch_size, \n",
        "                                          num_workers=num_workers)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
            "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
            "Using downloaded and verified file: data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
            "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
            "Using downloaded and verified file: data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
            "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c042cf672ea4171a9b970f6e2fa704b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Training Data\n",
        "\n",
        "The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data. \n",
        "> Each image is `28` pixels in width and height and `1` in depth. I'll sometimes write this as an image with dimensions `(28, 28, 1)`.\n",
        "\n",
        "If you recall, the depth is just the number of color channels in an image, so 1 is the depth for a grayscale image and 3 is the depth for an RGB image."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "    \n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    # print out the correct label for each image\n",
        "    # .item() gets the value contained in a Tensor\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x288 with 20 Axes>"
            ],
            "image/png": [
              "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\n"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Encoder\n",
        "\n",
        "The encoder is made of a series of layers that are responsible for taking in as input a 28 by 28 MNIST image and learning to encode it into a 16-dimensional output vector.\n",
        "\n",
        "<img src='assets/capsule_encoder.png' width=70%/>\n",
        "\n",
        "(Image from [the original Capsule Network paper](https://arxiv.org/pdf/1710.09829.pdf))\n",
        "\n",
        "## First Layer: Convolutional Layer\n",
        "\n",
        "The first layer in our encoder is a convolutional layer that will learn to extract features, like edges, in a given input image. \n",
        "> In the original paper, this first convolutional layer has a `depth=256`, and uses a `kernel_size=9`, `stride=1`, and `padding=0`. This also calls for a `ReLu` activation on the outputs.\n",
        "\n",
        "So, this convolutional layer will create a stack of 256 filtered images, given one input MNIST image. \n",
        "\n",
        "For a 28x28 input image, a 9x9 kernel will not be able to perfectly overlay on the edges pixels of an image and, without any padding, I'll actually lose a border of 4 pixels on *each* side of the image as I filter it. So, for an input of size `(batch_size, 28, 28, 1)`, I'll get an output, convolutional layer with the dimensions `(batch_size, 20, 20, 256)`. And `batch_size` is just the number of input images that are processed in a batch; defined above in the `DataLoader` as `20`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, out_channels=256):\n",
        "        '''Constructs the ConvLayer with a specified input and output size.\n",
        "           param in_channels: input depth of an image, default value = 1\n",
        "           param out_channels: output depth of the convolutional layer, default value = 256\n",
        "           '''\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        # defining a convolutional layer of the specified size\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
        "                              kernel_size=9, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param x: the input to the layer; an input image\n",
        "           return: a relu-activated, convolutional layer\n",
        "           '''\n",
        "        # applying a ReLu activation to the outputs of the conv layer\n",
        "        features = F.relu(self.conv(x)) # will have dimensions (batch_size, 20, 20, 256)\n",
        "        return features\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Layer: Primary Capsules\n",
        "\n",
        "This layer is composed of 8 \"primary\" capsules, so called because they are the first layer of capsules. They take the output of the previously defined, convolutional layer as input and produce a set of output vectors. Essentially, each capsule is responsible for producing weighted combinations of the features detected in the previous convolutional layer. \n",
        "\n",
        "As per the paper, to get the output of one capsule, I'll:\n",
        ">* Define a convolutional layer with a `depth=32`, `kernel_size=9`, `**stride=2**`, and `padding=0` and apply it to the features from the previous layer.\n",
        "* By applying a convolutional layer to features of size `(batch_size, 20, 20, 256)`, I will get a `(batch_size, 6, 6, 32)` Tensor as output for each of the 8 capsules. \n",
        "* I'll **reshape** this into an output vector using `.view(batch_size, 32*6*6, 1)`, for each of our 8 capsules and then stack those vectors.\n",
        "* Finally, I'll **squash** all of these vector; this is a nonlinear normalization step that forces the magnitude of the vectors to be in the range 0-1. \n",
        "\n",
        "#### A note on calculating dimensions\n",
        "\n",
        "You may be wondering how I got from 20x20 to 6x6.\n",
        "\n",
        "Again, I'm creating a border of 4 pixels on each side with a kernel_size of 9. So, I'll go from an x-y size of 20 - 4 on *all* sides to a size of 12. Then, with a `stride=2`, I'm efectively downsampling the previous inputs by a factor of 2, so I go from an x-y size of 12, divided by 2, to get an x-y size of 6. And you can read more about these precise output dimension calculations [at this Stanford cs231n page](http://cs231n.github.io/convolutional-networks/#conv) in the section on **spatial arrangement**.\n",
        "\n",
        "### Squashing\n",
        "\n",
        "> The magnitude of an output, capsule vector is a value between 0 and 1 that indicates the probability that a visual part (eyes, nose, etc.) exists and has been detected in an image. \n",
        "\n",
        "To get this probability value, I define a nonlinear function `squash` that calculates a certain capsule's normalized, vector output using the following equation. \n",
        "\n",
        "$$ v_j = \\frac{||\\ s_j^2\\ ||\\ s_j }{1 + ||\\ s_j^2\\ ||\\ s_j } $$\n",
        "\n",
        "$v_j$ is the value I want to calculate, the normalized vector output of a capsule $j$. And $s_j$ is that capsule's total input; a weighted sum over all the output vectors from the capsules in the layer *below* capsule $j$. For all but the primary capsule layer, you can think of the weighted inputs as the number of smaller *parts* that think they are part of a larger whole. The idea is that if you detect a nose and eyes in one layer, with some probability (the magnitude of a vector), this is evidence that a larger face exists in the next layer.\n",
        "\n",
        "> The `squash` function is a nonlinear function of the weighted inputs to a single capsule. It ensures that the magnitude of $v_j$ is a value between 0 and 1.\n",
        "\n",
        "#### ModuleList\n",
        "\n",
        "Below, I'm taking advantage of PyTorch's [nn.ModuleList](https://pytorch.org/docs/stable/nn.html#modulelist) container to create a list of convolutional layers that is as long as my number of passed in capsules, `num_capsules`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimaryCaps(nn.Module):\n",
        "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32):\n",
        "        '''Constructs a list of convolutional layers to be used in \n",
        "           creating capsule output vectors.\n",
        "           param num_capsules: number of capsules to create\n",
        "           param in_channels: input depth of features, default value = 256\n",
        "           param out_channels: output depth of the convolutional layers, default value = 32\n",
        "           '''\n",
        "        super(PrimaryCaps, self).__init__()\n",
        "        # creating a list of convolutional layers for each capsule I want to create\n",
        "        # all capsules have a conv layer with the same parameters\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
        "                      kernel_size=9, stride=2, padding=0)\n",
        "            for _ in range(num_capsules)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param x: the input; features from a convolutional layer\n",
        "           return: a set of normalized, capsule output vectors\n",
        "           '''\n",
        "        # get batch size of inputs\n",
        "        batch_size = x.size(0)\n",
        "        # reshape convolutional layer outputs to be (batch_size, vector_dim=1152, 1)\n",
        "        u = [capsule(x).view(batch_size, 32 * 6 * 6, 1) for capsule in self.capsules]\n",
        "        # stack up output vectors, u, one for each capsule\n",
        "        u = torch.cat(u, dim=-1)\n",
        "        # squashing the stack of vectors\n",
        "        u_squash = self.squash(u)\n",
        "        return u_squash\n",
        "\n",
        "    def squash(self, input_tensor):\n",
        "        '''Squashes an input Tensor so it has a magnitude between 0-1.\n",
        "           param input_tensor: a stack of capsule inputs, s_j\n",
        "           return: a stack of normalized, capsule output vectors, v_j\n",
        "           '''\n",
        "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm) # normalization coeff\n",
        "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)    \n",
        "        return output_tensor"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Third Layer: Digit Capsules\n",
        "\n",
        "This layer is composed of 10 \"digit\" capsules, one for each of our digit classes 0-9. Each capsule takes, as input, a batch of 1152-dimensional vectors produced by our 8 primary capsules, above. \n",
        "\n",
        "> This is the final layer of the encoder portion of the network, and each of these 10 capsules is responsible for producing a 16-dimensional output vector.\n",
        "\n",
        "In other words, this layer is mapping from 1152-dimensional, input vector space to 16-dimensional, output vector space. This is also where you'll see **dynamic routing** implemented."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Routing\n",
        "\n",
        "Dynamic routing is a process for finding the *best* connections between the output of one layer of capsules and the inputs of the next layer of capsules. It allows capsules to communicate with each other and determine how data moves through them! \n",
        "> No matter what kind of input image a capsule network sees, dynamic routing ensures that the output of a *child* capsule in the `PrimaryCaps` layer will be sent to the most-relevant *parent* capsule in this `DigitCaps` layer.\n",
        "\n",
        "### Coupling Coefficients\n",
        "\n",
        "When a capsule network is initialized, the primary capsules are not sure which parents (digit capsules) their outputs should go to. In fact, each primary capsule starts out with a list of **possible parents** that starts out as *all* of the parent capsules in the next layer. \n",
        "\n",
        "> This possible relationship is represented by a value called the **coupling coefficient**, **c**, which is the probability that a certain capsule’s output should go to a parent capsule in the next layer. \n",
        "\n",
        "Examples of coupling coefficients, written on the connecting lines between a child and its possible parent nodes, are pictured below. A child node with two possible parents will start out with equal coupling coefficients for both: 0.5.\n",
        "\n",
        "<img src='assets/coupling_coeff.png' width=30%/>\n",
        "\n",
        "(Image from [my blog post on Capsule Nets](https://cezannec.github.io/Capsule_Networks/))\n",
        "\n",
        "The coupling coefficients across all possible parents can be pictured as a discrete probability distribution. Across all connections between one child capsule and all possible parent capsules, the coupling coefficients should sum to 1.\n",
        "\n",
        "\n",
        "### Routing by Agreement\n",
        "\n",
        "Dynamic routing is an iterative process that updates these coupling coefficients. The coupling coefficients $c_{ij}$ between capsule $i$ and all the capsules $j$ in the layer above it sum to 1 and are determined by a routing **softmax** function whose initial logits $b_{ij}$ are the log prior probabilities that capsule $i$ *should* be coupled to capsule $j$. There are as many $b_{ij}$ as there are possible connections between the output vector of a primary capsule (length 1152) and digit capsules (10).\n",
        "\n",
        "\n",
        "$$c_{ij} = \\frac{e^{\\ b_{ij}}}{\\sum_{k}\\ {e^{\\ b_{ik}}}} $$\n",
        "\n",
        "The update process, performed during network training, is as follows for a single capsule:\n",
        "\n",
        "> 1. Every child capsule will output some vector $u$, whose magnitude indicates a part's *existence* and whose orientation represents the generalized pose of the part.\n",
        "2. For each possible parent, a child capsule computes a prediction vector, $\\hat{u}$, which is a function of its output vector, $u$, times a weight matrix, $W$. $W$ represents a linear transformation in space—like some rotation and translation—$\\hat{u}$ can be thought of as a prediction about the position and orientation of a parent capsule's output vector.\n",
        "3. If the prediction vector, $\\hat{u}$, has a large **dot product** with the parent capsule output vector, $v$, then those vectors are said to **agree** and the coupling coefficient between *that* parent and the child capsule increases while the coupling coefficient between that child capsule and *all other* parents, decreases.\n",
        "4. This dot product between the parent output vector, $v$, and a prediction vector, $\\hat{u}$, is known as a formal measure of capsule **agreement**, $a$.\n",
        "5. This agreement then affects how informaton is weighted as it moved through the network.\n",
        "6. A new $b_{ij}$ is calculated as the *current* $b_{ij}$ plus $a$.\n",
        "7. New coupling coefficients $c_{ij}$ are calculated using the above softmax formula and the new $b_{ij}$.\n",
        "\n",
        "$$\\hat{u} = W u $$\n",
        "\n",
        "$$a = v \\cdot u $$\n",
        "\n",
        "$$b_{ij} = b_{ij} + a $$\n",
        "\n",
        "\n",
        "\n",
        "This is sometimes referred to as **top-down feedback**; feedback from a later layer of parent capsule outputs.\n",
        "\n",
        "**A typical training process may include *three* agreement iterations, in which $b_{ij}$ is updated (through steps 1-7), a total of three times to get to the final coupling coefficients that will be used to calculate final outputs, $v_j$.**\n",
        "\n",
        "> A high coupling coefficient, between a child and parent capsule, increases the contribution of the child to that parent, thus *further* aligning their two output vectors and making their agreement dot product even larger! This is called **routing by agreement**.\n",
        "\n",
        "I should note that this calculation changes how I calculate the total inputs to a digit capsule, $s_j$, when compared to the earlier, `PrimaryCaps` layer. In the `DigitCaps` case, $s_j$ is a sum of all the input vectors from the capsules in the layer *below*, $u$ times the weight matrix $W$, in other words $W u = \\hat{u}$, *also* multiplied by the coupling coefficients. The first layer of capsules sees only convolutional features as input and these features do not provide enough information for routing and using couping coefficients.\n",
        "\n",
        "$$ s_j = \\sum{c_{ij} \\ \\hat{u}}$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import helpers # to get transpose softmax function\n",
        "\n",
        "# dynamic routing\n",
        "def dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\n",
        "    '''Performs dynamic routing between two capsule layers.\n",
        "       param b_ij: initial log probabilities that capsule i should be coupled to capsule j\n",
        "       param u_hat: input, weighted capsule vectors, W u\n",
        "       param squash: given, normalizing squash function\n",
        "       param routing_iterations: number of times to update coupling coefficients\n",
        "       return: v_j, output capsule vectors\n",
        "       '''\n",
        "    # update b_ij, c_ij for number of routing iterations\n",
        "    for iteration in range(routing_iterations):\n",
        "        # softmax calculation of coupling coefficients, c_ij\n",
        "        c_ij = helpers.softmax(b_ij, dim=2)\n",
        "\n",
        "        # calculating total capsule inputs, s_j = sum(c_ij*u_hat)\n",
        "        s_j = (c_ij * u_hat).sum(dim=2, keepdim=True)\n",
        "\n",
        "        # squashing to get a normalized vector output, v_j\n",
        "        v_j = squash(s_j)\n",
        "\n",
        "        # if not on the last iteration, calculate agreement and new b_ij\n",
        "        if iteration < routing_iterations - 1:\n",
        "            # agreement\n",
        "            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\n",
        "\n",
        "            # new b_ij\n",
        "            b_ij = b_ij + a_ij\n",
        "\n",
        "    return v_j # return latest v_j    "
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Digit Capsules\n",
        "\n",
        "Now that I've defined the dynamic routing process, I can complete the `DigitCaps` class.\n",
        "* This layer is composed of 10 \"digit\" capsules, one for each of our digit classes 0-9. \n",
        "* Each capsule takes, as input, a batch of 1152-dimensional vectors produced by our 8 primary capsules, above. \n",
        "* Each of these 10 capsules is responsible for producing a **16-dimensional** output vector.\n",
        "\n",
        "I suggest you train on GPU for a faster training time, and I'm checking for the availability of a GPU device, below."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# it will also be relevant, in this model, to see if I can train on gpu\n",
        "TRAIN_ON_GPU = torch.cuda.is_available()\n",
        "\n",
        "if(TRAIN_ON_GPU):\n",
        "    print('Training on GPU!')\n",
        "else:\n",
        "    print('Only CPU available')\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class DigitCaps(nn.Module):\n",
        "\n",
        "    def __init__(self, num_capsules=10, previous_layer_nodes=32*6*6, \n",
        "                 in_channels=8, out_channels=16):\n",
        "        '''Constructs an initial weight matrix, W, and sets class variables.\n",
        "           param num_capsules: number of capsules to create\n",
        "           param previous_layer_nodes: dimension of input capsule vector, default value = 1152\n",
        "           param in_channels: number of capsules in previous layer, default value = 8\n",
        "           param out_channels: dimensions of output capsule vector, default value = 16\n",
        "           '''\n",
        "        super(DigitCaps, self).__init__()\n",
        "\n",
        "        # setting class variables\n",
        "        self.num_capsules = num_capsules\n",
        "        self.previous_layer_nodes = previous_layer_nodes # vector input (dim=1152)\n",
        "        self.in_channels = in_channels # previous layer's number of capsules\n",
        "\n",
        "        # starting out with a randomly initialized weight matrix, W\n",
        "        # these will be the weights connecting the PrimaryCaps and DigitCaps layers\n",
        "        self.W = nn.Parameter(torch.randn(num_capsules, previous_layer_nodes, \n",
        "                                          in_channels, out_channels))\n",
        "\n",
        "    def forward(self, u):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param u: the input; vectors from the previous PrimaryCaps layer\n",
        "           return: a set of normalized, capsule output vectors\n",
        "           '''\n",
        "\n",
        "        # adding batch_size dims and stacking all u vectors\n",
        "        u = u[None, :, :, None, :]\n",
        "        # 4D weight matrix\n",
        "        W = self.W[:, None, :, :, :]\n",
        "        \n",
        "        # calculating u_hat = W*u\n",
        "        u_hat = torch.matmul(u, W)\n",
        "\n",
        "        # getting the correct size of b_ij\n",
        "        # setting them all to 0, initially\n",
        "        b_ij = torch.zeros(*u_hat.size())\n",
        "        \n",
        "        # moving b_ij to GPU, if available\n",
        "        if TRAIN_ON_GPU:\n",
        "            b_ij = b_ij.cuda()\n",
        "\n",
        "        # update coupling coefficients and calculate v_j\n",
        "        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=3)\n",
        "\n",
        "        return v_j # return final vector outputs\n",
        "    \n",
        "    \n",
        "    def squash(self, input_tensor):\n",
        "        '''Squashes an input Tensor so it has a magnitude between 0-1.\n",
        "           param input_tensor: a stack of capsule inputs, s_j\n",
        "           return: a stack of normalized, capsule output vectors, v_j\n",
        "           '''\n",
        "        # same squash function as before\n",
        "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm) # normalization coeff\n",
        "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)    \n",
        "        return output_tensor\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This completes the encoder portion of the model! Next, the decoder.\n",
        "\n",
        "---\n",
        "# Decoder \n",
        "\n",
        "The decoder sees as input the 16-dimensional vectors that are produced by the `DigitCaps` layer. In the `forward` function, the decoder identifies the \"correct\" capsule output vector; this vector is the vector with the largest vector magnitude of all ten digit capsule outputs (recall that vector magnitude correspond to a part's **existence** in an image). Then, the decoder upsamples that one vector, decoding it into a reconstructed image of a handwritten digit. \n",
        "\n",
        "> The decoder is learning a mapping from a capsule output vector to a 784-dim vector that can be reshaped into a 28x28 reconstructed image. \n",
        "\n",
        "<img src='assets/capsule_decoder.png' width=60% />\n",
        "\n",
        "(Image from [the original Capsule Network paper](https://arxiv.org/pdf/1710.09829.pdf))\n",
        "\n",
        "I want this reconstructed image and the original, input image to be as close as possible! So, when I train this network, I'll look at the difference between the input image and this decoder reconstruction; any difference (measured as Euclidean distance) will be recorded as the loss.\n",
        "\n",
        "#### Reconstruction Loss\n",
        "I'll discuss the network loss in more detail later. For now, know that the authors use this decoder-based, reconstruction loss to encourage the digit capsules to learn to encode information about the content of the original input image.\n",
        "\n",
        "## Linear Layers\n",
        "\n",
        "The decoder is made of three fully-connected, linear layers. These are simply defined with an input_size and output_size. The first layer sees the 10, 16-dimensional output vectors from the digit capsule layer and produces `hidden_dim=512` number of outputs. The next hidden layer just increase this depth by two, and the third and final linear layer produces an output of 784 values that can be reshaped into a 28x28 image!\n",
        "\n",
        "The hidden layers have ReLu activations applied and the final, output layer has a **sigmoid** activation function to ensure that the outputs are all values in a valid pixel range 0-1."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_vector_length=16, input_capsules=10, hidden_dim=512):\n",
        "        '''Constructs an series of linear layers + activations.\n",
        "           param input_vector_length: dimension of input capsule vector, default value = 16\n",
        "           param input_capsules: number of capsules in previous layer, default value = 10\n",
        "           param hidden_dim: dimensions of hidden layers, default value = 512\n",
        "           '''\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # calculate input_dim\n",
        "        input_dim = input_vector_length * input_capsules\n",
        "        \n",
        "        # define linear layers + activations\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim), # first hidden layer\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim*2), # second, twice as deep\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim*2, 28*28), # can be reshaped into 28*28 image\n",
        "            nn.Sigmoid() # sigmoid activation to get output pixel values in a range from 0-1\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param x: the input; vectors from the previous DigitCaps layer\n",
        "           return: two things, reconstructed images and the class scores, y\n",
        "           '''\n",
        "        classes = (x ** 2).sum(dim=-1) ** 0.5\n",
        "        classes = F.softmax(classes, dim=-1)\n",
        "        \n",
        "        # find the capsule with the maximum vector length\n",
        "        # here, vector length indicates the probability of a class' existence\n",
        "        _, max_length_indices = classes.max(dim=1)\n",
        "        \n",
        "        # create a sparse class matrix\n",
        "        sparse_matrix = torch.eye(10) # 10 is the number of classes\n",
        "        if TRAIN_ON_GPU:\n",
        "            sparse_matrix = sparse_matrix.cuda()\n",
        "        # get the class scores from the \"correct\" capsule\n",
        "        y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n",
        "        \n",
        "        # create reconstructed pixels\n",
        "        x = x * y[:, :, None]\n",
        "        # flatten image into a vector shape (batch_size, vector_dim)\n",
        "        flattened_x = x.view(x.size(0), -1)\n",
        "        # create reconstructed image vectors\n",
        "        reconstructions = self.linear_layers(flattened_x)\n",
        "        \n",
        "        # return reconstructions and the class scores, y\n",
        "        return reconstructions, y"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put it All Together \n",
        "\n",
        "Finally, I'll use *all* the layers I defined above to create a complete Capsule Network! Recall that the order of these layers is as follows:\n",
        "1. ConvLayer\n",
        "2. PrimaryCaps\n",
        "3. DigitCaps\n",
        "4. Decoder\n",
        "\n",
        "And I specified all the default construction parameters.\n",
        "\n",
        "<img src='assets/complete_caps_net.png' width=80%/>\n",
        "\n",
        "(Image from [the original Capsule Network paper](https://arxiv.org/pdf/1710.09829.pdf))"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class CapsuleNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''Constructs a complete Capsule Network.'''\n",
        "        super(CapsuleNetwork, self).__init__()\n",
        "        self.conv_layer = ConvLayer()\n",
        "        self.primary_capsules = PrimaryCaps()\n",
        "        self.digit_capsules = DigitCaps()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, images):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param images: the original MNIST image input data\n",
        "           return: output of DigitCaps layer, reconstructed images, class scores\n",
        "           '''\n",
        "        primary_caps_output = self.primary_capsules(self.conv_layer(images))\n",
        "        caps_output = self.digit_capsules(primary_caps_output).squeeze().transpose(0,1)\n",
        "        reconstructions, y = self.decoder(caps_output)\n",
        "        return caps_output, reconstructions, y\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Complete Model\n",
        "\n",
        "I'm printing it out to see that I've defined a model with convolutional layers, primary and digit capsule layers, and a decoder."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate and print net\n",
        "capsule_net = CapsuleNetwork()\n",
        "\n",
        "print(capsule_net)\n",
        "\n",
        "# move model to GPU, if available \n",
        "if TRAIN_ON_GPU:\n",
        "    capsule_net = capsule_net.cuda()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CapsuleNetwork(\n",
            "  (conv_layer): ConvLayer(\n",
            "    (conv): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
            "  )\n",
            "  (primary_capsules): PrimaryCaps(\n",
            "    (capsules): ModuleList(\n",
            "      (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (1): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (2): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (3): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (4): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (5): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (6): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (7): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "    )\n",
            "  )\n",
            "  (digit_capsules): DigitCaps()\n",
            "  (decoder): Decoder(\n",
            "    (linear_layers): Sequential(\n",
            "      (0): Linear(in_features=160, out_features=512, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Linear(in_features=1024, out_features=784, bias=True)\n",
            "      (5): Sigmoid()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Loss\n",
        "\n",
        "To prepare for training the model, I'll need to define a custom loss. The loss for a capsule network is a weighted combination of **two** losses:\n",
        "1. The **margin loss** that looks at the outputs of *all* the digit capsules and computes a loss term that is dependent on whether or not the selected \"correct\" digit capsule matches the correct class label.\n",
        "2. The **reconstruction loss** that looks at the original input images and how they differ from the decoder's produced, reconstructed images. \n",
        "\n",
        "The reconstruction loss is a simple mean squared error loss, which I will calculate as `nn.MSELoss`. And I explained that this acts as a kind of [regularization method](https://en.wikipedia.org/wiki/Regularization_(mathematics)), above. By regularization, I mean that it forces the 16 dimensions (in the output vectors from the `DigitCaps` layer) to correspond to meaningful content information; information that can then be used to *reconstruct* the original image. It is important to note that the authors of the original Capsule Net paper weighted this loss with a coefficient of `0.0005`, so that it wouldn't overpower the margin loss, and that is reflected in the code below.\n",
        "\n",
        "So, I really want to focus on describing this new type of loss, the margin loss.\n",
        "\n",
        "### Margin Loss\n",
        "\n",
        "Margin loss is a classification loss that is based on the length of the output vectors coming from the `DigitCaps` layer. Recall that the magnitude of a capsule output vector is a measure of a part's **existence**. So, for any input image of a handwritten digit, we want *one* particular digit capsule to output a long vector, indicating confidence in one particular digit class! This capsule is sometimes referred to as the \"correct\" capsule.\n",
        "\n",
        "Margin loss formalizes this idea; it says that if a certain digit (in this case a digit 0-9) is present in an image then the squared length of the corresponding output vector of that digit capsule must not be less than 0.9. So, if I have an input image of a 0, then the \"correct,\" zero-detecting, digit capsule should output a vector of magnitude 0.9 or greater! For all the other digits (1-9, in this example) the corresponding digit capsule output vectors should have a magnitude that is 0.1 or less.\n",
        "\n",
        "Suppose $x$ is the the output vector of the digit capsule for class $c$ (0-9). \n",
        "\n",
        "Now, if an image of class $c$ is present then the magnitude of $x$ should be equal to or greater than 0.9. The magnitude is the square root of a squared value, and I'll call that magnitude $v_c = \\sqrt{x^2}$, so $v_c >=0.9$. Similarly, if the class $c$ digit is *not* present in an image, then $v_c <=0.1$.  These values will be summed together to calculate the total, margin loss. Which you can see in the following formula.\n",
        "\n",
        "$$L_c = T_c(\\max[0, 0.9-v_c]) + \\lambda (1-T_c)(\\max[0, v_c-0.1])$$\n",
        "\n",
        "Where $T_c$ is a label 0 or 1 indicating whether a digit capsule vector is \"correct\" (1) or not (0). And $\\lambda$ is a value we choose for training; the researchers chose a value of `0.5`. These choices are reflected in the code below, *and* I'm using a ReLu function to get the maximum of 0 and another value.\n",
        "\n",
        "\n",
        "This loss will encourage our model to make the index of the \"correct\" capsule match the true class label of a given training image.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class CapsuleLoss(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        '''Constructs a CapsuleLoss module.'''\n",
        "        super(CapsuleLoss, self).__init__()\n",
        "        self.reconstruction_loss = nn.MSELoss(reduction='sum') # cumulative loss, equiv to size_average=False\n",
        "\n",
        "    def forward(self, x, labels, images, reconstructions):\n",
        "        '''Defines how the loss compares inputs.\n",
        "           param x: digit capsule outputs\n",
        "           param labels: \n",
        "           param images: the original MNIST image input data\n",
        "           param reconstructions: reconstructed MNIST image data\n",
        "           return: weighted margin and reconstruction loss, averaged over a batch\n",
        "           '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        ##  calculate the margin loss   ##\n",
        "        \n",
        "        # get magnitude of digit capsule vectors, v_c\n",
        "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
        "\n",
        "        # calculate \"correct\" and incorrect loss\n",
        "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
        "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
        "        \n",
        "        # sum the losses, with a lambda = 0.5\n",
        "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
        "        margin_loss = margin_loss.sum()\n",
        "\n",
        "        ##  calculate the reconstruction loss   ##\n",
        "        images = images.view(reconstructions.size()[0], -1)\n",
        "        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
        "\n",
        "        # return a weighted, summed loss, averaged over a batch size\n",
        "        return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify Loss Function and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
        "\n",
        "I'm using the custom loss I defined above, and the paper uses an Adam optimizer."
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# custom loss\n",
        "criterion = CapsuleLoss()\n",
        "\n",
        "# Adam optimizer with default params\n",
        "optimizer = optim.Adam(capsule_net.parameters())"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Train the Network\n",
        "\n",
        "The steps for training/learning from a batch of data are described in the comments below:\n",
        "1. Clear the gradients of all optimized variables\n",
        "2. Forward pass: compute predicted outputs by passing inputs to the model\n",
        "3. Calculate the loss\n",
        "4. Backward pass: compute gradient of the loss with respect to model parameters\n",
        "5. Perform a single optimization step (parameter update)\n",
        "6. Update average training loss\n",
        "\n",
        "The following loop trains for some number of epochs and prints out the training loss intermittently; take a look at how the values for training loss decrease over time."
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(capsule_net, criterion, optimizer, \n",
        "          n_epochs, print_every=300):\n",
        "    '''Trains a capsule network and prints out training batch loss statistics.\n",
        "       Saves model parameters if *validation* loss has decreased.\n",
        "       param capsule_net: trained capsule network\n",
        "       param criterion: capsule loss function\n",
        "       param optimizer: optimizer for updating network weights\n",
        "       param n_epochs: number of epochs to train for\n",
        "       param print_every: batches to print and save training loss, default = 100\n",
        "       return: list of recorded training losses\n",
        "       '''\n",
        "\n",
        "    # track training loss over time\n",
        "    losses = []\n",
        "\n",
        "    # one epoch = one pass over all training data \n",
        "    for epoch in range(1, n_epochs+1):\n",
        "\n",
        "        # initialize training loss\n",
        "        train_loss = 0.0\n",
        "        \n",
        "        capsule_net.train() # set to train mode\n",
        "    \n",
        "        # get batches of training image data and targets\n",
        "        for batch_i, (images, target) in enumerate(train_loader):\n",
        "\n",
        "            # reshape and get target class\n",
        "            target = torch.eye(10).index_select(dim=0, index=target)\n",
        "\n",
        "            if TRAIN_ON_GPU:\n",
        "                images, target = images.cuda(), target.cuda()\n",
        "\n",
        "            # zero out gradients\n",
        "            optimizer.zero_grad()\n",
        "            # get model outputs\n",
        "            caps_output, reconstructions, y = capsule_net(images)\n",
        "            # calculate loss\n",
        "            loss = criterion(caps_output, target, images, reconstructions)\n",
        "            # perform backpropagation and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() # accumulated training loss\n",
        "            \n",
        "            # print and record training stats\n",
        "            if batch_i != 0 and batch_i % print_every == 0:\n",
        "                avg_train_loss = train_loss/print_every\n",
        "                losses.append(avg_train_loss)\n",
        "                print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\n",
        "                train_loss = 0 # reset accumulated training loss\n",
        "        \n",
        "    return losses\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training for 3 epochs\n",
        "n_epochs = 3\n",
        "losses = train(capsule_net, criterion, optimizer, n_epochs=n_epochs)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-16-ce644b7b7998>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# training for 3 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-15-54eb5db28cd7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(capsule_net, criterion, optimizer, n_epochs, print_every)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# get model outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcapsule_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-11-1bf5514185a1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprimary_caps_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary_capsules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcaps_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigit_capsules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary_caps_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaps_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-10-ffae35828f12>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# flatten image into a vector shape (batch_size, vector_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mflattened_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m# create reconstructed image vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mreconstructions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflattened_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loss\n",
        "\n",
        "Here I'll plot the training loss that was recorded intermittently, as specified by `print_every`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'losses' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-17-5586f75db466>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Test the Trained Network\n",
        "\n",
        "In this case, there are two ways to test the trained model: looking at how it classifies test data, and looking at how its reconstructions compare to original input images.\n",
        "\n",
        "## Test Data\n",
        "First, I'll test this model on previously unseen **test data** and evaluate its performance. Testing on unseen data is a good way to check that our model generalizes well. It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def test(capsule_net, test_loader):\n",
        "    '''Prints out test statistics for a given capsule net.\n",
        "       param capsule_net: trained capsule network\n",
        "       param test_loader: test dataloader\n",
        "       return: returns last batch of test image data and corresponding reconstructions\n",
        "       '''\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    \n",
        "    test_loss = 0 # loss tracking\n",
        "\n",
        "    capsule_net.eval() # eval mode\n",
        "\n",
        "    for batch_i, (images, target) in enumerate(test_loader):\n",
        "        target = torch.eye(10).index_select(dim=0, index=target)\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        if TRAIN_ON_GPU:\n",
        "            images, target = images.cuda(), target.cuda()\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        caps_output, reconstructions, y = capsule_net(images)\n",
        "        # calculate the loss\n",
        "        loss = criterion(caps_output, target, images, reconstructions)\n",
        "        # update average test loss \n",
        "        test_loss += loss.item()\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(y.data.cpu(), 1)\n",
        "        _, target_shape = torch.max(target.data.cpu(), 1)\n",
        "\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target_shape.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(batch_size):\n",
        "            label = target_shape.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "    # avg test loss\n",
        "    avg_test_loss = test_loss/len(test_loader)\n",
        "    print('Test Loss: {:.8f}\\n'.format(avg_test_loss))\n",
        "\n",
        "    for i in range(10):\n",
        "        if class_total[i] > 0:\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "                str(i), 100 * class_correct[i] / class_total[i],\n",
        "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "        100. * np.sum(class_correct) / np.sum(class_total),\n",
        "        np.sum(class_correct), np.sum(class_total)))\n",
        "    \n",
        "    # return last batch of capsule vectors, images, reconstructions\n",
        "    return caps_output, images, reconstructions"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call test function and get reconstructed images\n",
        "caps_output, images, reconstructions = test(capsule_net, test_loader)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-19-9d3576a29cc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# call test function and get reconstructed images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-18-839b6ea3f62a>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(capsule_net, test_loader)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcapsule_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m# calculate the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-11-1bf5514185a1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprimary_caps_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary_capsules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcaps_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigit_capsules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary_caps_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaps_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-10-ffae35828f12>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# flatten image into a vector shape (batch_size, vector_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mflattened_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m# create reconstructed image vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mreconstructions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflattened_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Reconstructions\n",
        "\n",
        "Second, I'll display original images and their reconstructions to see how well the decoder trained."
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(images, reconstructions):\n",
        "    '''Plot one row of original MNIST images and another row (below) \n",
        "       of their reconstructions.'''\n",
        "    # convert to numpy images\n",
        "    images = images.data.cpu().numpy()\n",
        "    reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
        "    reconstructions = reconstructions.data.cpu().numpy()\n",
        "    \n",
        "    # plot the first ten input images and then reconstructed images\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(26,5))\n",
        "\n",
        "    # input images on top row, reconstructions on bottom\n",
        "    for images, row in zip([images, reconstructions], axes):\n",
        "        for img, ax in zip(images, row):\n",
        "            ax.imshow(np.squeeze(img), cmap='gray')\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display original and reconstructed images, in rows\n",
        "display_images(images, reconstructions)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'reconstructions' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-21-bbc192282dbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# display original and reconstructed images, in rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdisplay_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'reconstructions' is not defined"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can see that the reconstructions look pretty good! Almost the same as the original images, with a little blurring/smoothing at the edges."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Response to Affine Transformations\n",
        "\n",
        "In the [capsule network paper](https://arxiv.org/pdf/1710.09829.pdf), they have a section that compares the performance of a capsule network and a vanilla CNN. One observation is that a capsule network is more robust to affine transformations in data. That is, even when the network is trained on normal MNIST data, if you then translate or rotate some test data, a trained CapsNet will perform better than a vanilla CNN.\n",
        "\n",
        "From the paper:\n",
        "> Experiments show that each DigitCaps capsule learns a more robust representation for each class than a traditional convolutional network. Because there is natural variance in skew, rotation, style, etc. in hand written digits, the trained CapsNet is moderately robust to small affine transformations of the training data.\n",
        "\n",
        "Their process for creating affine-transformed test data is different than mine, but I wanted to see how well my trained capsule network performed on randomly, affine-transformed data. Below, I am applying an affine transformation that will randomly rotate and image between 0-30 degrees as well as vertically/horizontally translate an image by 10% of the total image height/width."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to Tensor *and* perform random affine transformation\n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomAffine(degrees=30, translate=(0.1,0.1)),\n",
        "     transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "# test dataset\n",
        "transformed_test_data = datasets.MNIST(root='data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# prepare data loader\n",
        "transformed_test_loader = torch.utils.data.DataLoader(transformed_test_data, \n",
        "                                                      batch_size=batch_size,\n",
        "                                                      num_workers=num_workers)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize transformed data\n",
        "\n",
        "You can see that this test data has been randomly rotated and translated."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(transformed_test_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    # print out the correct label for each image\n",
        "    # .item() gets the value contained in a Tensor\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x288 with 20 Axes>"
            ],
            "image/png": [
              "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\n"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on transformed test data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# call test function and get reconstructed images\n",
        "_, images, reconstructions = test(capsule_net, transformed_test_loader)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-24-434732f67bfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# call test function and get reconstructed images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_test_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-18-839b6ea3f62a>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(capsule_net, test_loader)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcapsule_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m# calculate the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-11-1bf5514185a1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprimary_caps_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary_capsules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcaps_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigit_capsules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary_caps_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaps_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-10-ffae35828f12>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# flatten image into a vector shape (batch_size, vector_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mflattened_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m# create reconstructed image vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mreconstructions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflattened_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# original input images\n",
        "display_images(images, reconstructions)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'reconstructions' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-25-53391e09ddfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# original input images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdisplay_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'reconstructions' is not defined"
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the accuracy is not as good as before, but it did not degrade terribly. It's really interesting to look at the reconstructions to see what mistakes this model is making!\n",
        "\n",
        "* I should note that I did this same test with a vanilla CNN, and got comparable results. The CNN got 85% test accuracy, so I'll need to do a more rigorous test to reveal any major differences between the two models."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capsule Output Vectors\n",
        "\n",
        "Recall that the `DigitCaps` layer outputs vectors of dim 16. It turns out that some of these vector dimensions have learned something about the features that make up each digit. From the paper:\n",
        "> Since we are passing the encoding of only one digit and zeroing out other digits, the dimensions of a digit capsule should learn to span the space of variations in the way digits of that class are instantiated.\n",
        "These variations include stroke thickness, skew and width. They also include digit-specific variations\n",
        "such as the length of the tail of a 2. **We can see what the individual dimensions represent by making\n",
        "use of the decoder network**. \n",
        "\n",
        "Here are some interesting findings from the paper:\n",
        "\n",
        "<img src='assets/perturbed_reconstructions.png' width=70% />\n",
        "\n",
        "Their method was to compute the output vector for a \"correct\" digit capsule and then modify each of the 16 vector dimensions by *perturbing* it; adding some small value in a range `[-0.25, 0.25]` by a factor of `0.05`. I try to replicate this experiment below, to see if I can find a certain vector dimension that corresponds to digit width, line-width, skew, or certain localized features!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_analysis(capsule_net, x, select_idx=1):\n",
        "    '''Generates perturbed iage reconstructions given some digit capsule outputs.\n",
        "       param capsule_net: trained capsule network\n",
        "       param x: a batch of digit capsule outputs\n",
        "       param select_idx: selects which image in a batch to analyze, default = 1 \n",
        "       return: list of perturbed, reconstructed images\n",
        "       '''\n",
        "    \n",
        "    classes = (x ** 2).sum(dim=-1) ** 0.5\n",
        "    classes = F.softmax(classes, dim=-1)\n",
        "\n",
        "    # find the capsule with the maximum vector length\n",
        "    # here, vector length indicates the probability of a class' existence\n",
        "    _, max_length_indices = classes.max(dim=1)\n",
        "\n",
        "    # create a sparse class matrix\n",
        "    sparse_matrix = torch.eye(10) # 10 is the number of classes\n",
        "    if TRAIN_ON_GPU:\n",
        "        sparse_matrix = sparse_matrix.cuda()\n",
        "    # get the class scores from the \"correct\" capsule\n",
        "    y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n",
        "\n",
        "    # create reconstructed pixels\n",
        "    x = x * y[:, :, None]\n",
        "    \n",
        "    # flatten image into a vector shape (batch_size, vector_dim)\n",
        "    flattened_x = x.view(x.size(0), -1)\n",
        "    # select a single image from a batch to work with\n",
        "    flattened_x = flattened_x[select_idx]\n",
        "    \n",
        "    # track reconstructed images\n",
        "    reconstructed_ims = []\n",
        "    # values to change *one* vector dimension by\n",
        "    perturb_range = np.arange(-0.25, 0.30, 0.05)\n",
        "    \n",
        "    # iterate through 16 vector dims\n",
        "    for k in range(16):\n",
        "        # create a copy of flattened_x to modify\n",
        "        transformed_x = torch.zeros(*flattened_x.size()).cuda()\n",
        "        transformed_x[:] = flattened_x[:]\n",
        "        # iterate through each perturbation value\n",
        "        for j in range(len(perturb_range)):\n",
        "            # for each capsule output\n",
        "            for i in range(10):\n",
        "                transformed_x[k+(16*i)] = flattened_x[k+(16*i)]+perturb_range[j]\n",
        "\n",
        "            # create reconstructed images\n",
        "            reconstructions = capsule_net.decoder.linear_layers(transformed_x)\n",
        "            # reshape into 28x28 image, (batch_size, depth, x, y)\n",
        "            reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
        "            reconstructed_ims.append(reconstructions)\n",
        "    \n",
        "    # return final list of reconstructed ims    \n",
        "    return reconstructed_ims\n",
        "        "
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call function and get perturbed reconstructions\n",
        "reconstructed_ims = vector_analysis(capsule_net, caps_output, select_idx=1)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'caps_output' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-27-00effa2ea7c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# call function and get perturbed reconstructions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreconstructed_ims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'caps_output' is not defined"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display the reconstructions\n",
        "\n",
        "The center image should be the original reconstruction; the five images to the left have negative perturbations and to the right there are 5 images with positive perturbations. \n",
        "\n",
        "* There are 16 rows of images, one for each of the vector dimensions\n",
        "* There are 11 images per row, one for each of the values in `perturb_range` defined above\n",
        "\n",
        "You should see that certain vector dims correspond to certain image properties; line width, skew, localized features, and so on! If you are running this code locally, I'd encourage you to try thin on a different image in a batch (using a different `select_idx`) and checkout the results."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 20)) # define figsize\n",
        "\n",
        "# display all ims\n",
        "for idx in range(len(reconstructed_ims)):\n",
        "    # convert to numpy images\n",
        "    image = reconstructed_ims[idx]\n",
        "    image = image.detach().cpu().numpy()\n",
        "    # display 16 rows of images\n",
        "    ax = fig.add_subplot(16, len(reconstructed_ims)/16, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(image.squeeze(), cmap='gray')\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'reconstructed_ims' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-28-46093a5d3f1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# display all ims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed_ims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# convert to numpy images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreconstructed_ims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'reconstructed_ims' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x1440 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further Directions\n",
        "\n",
        "The authors also note that this capsule network is able to perform fairly well in cases of image overlap. For example, it is able to classify two, individual handwritten digits even if they were drawn one on top of the other (images had an average of 80% overlap). This is a challenging computer vision problem that would be interesting to explore.\n",
        "\n",
        "I'd also recommend you take a look at the author's [code in Github](https://github.com/gram-ai/capsule-networks) where they also have an implementation that works with the SVHN dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}